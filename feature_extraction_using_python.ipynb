{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature extraction using python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMILZyAEkCvjuNlbck8jFG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivckr/Python-Jupyter_notebook/blob/master/feature_extraction_using_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOn1sE0gITea",
        "colab_type": "text"
      },
      "source": [
        "#loading dataste friom kaggle into colab\n",
        "Source: https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lXJk8B_JVzd",
        "colab_type": "text"
      },
      "source": [
        "We cannot work with the text data in machine learning so we need to convert them into numerical vectors\n",
        "\n",
        "Text data needs to be cleaned and encoded to numerical values before giving them to machine learning models, this process of cleaning and encoding is called as Text Preprocessing or vectorization of often featrure extraction.\n",
        "\n",
        "\n",
        "\n",
        "Understanding the data - See what's data is all about. what should be considered for cleaning for data (Punctuations , stopwords etc..).\n",
        "Basic Cleaning need to be considered for cleaning of data (like Punctuations , stopwords etc..) and its code.\n",
        "Techniques for Encoding - .\n",
        "Bag of Words\n",
        "\n",
        "1.   Binary Bag of Words\n",
        "2.   Bigram, Ngram\n",
        "3.   TF-IDF( Term Frequency - Inverse Document Frequency)\n",
        "4.   Word2Vec\n",
        "5.   Avg-Word2Vec\n",
        "6.   TF-IDF Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CRqcEJzrPm-",
        "colab_type": "code",
        "outputId": "8e0d6b64-999e-4a02-845b-29321b4658fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount the drive to colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I41d7jYjvaen",
        "colab_type": "code",
        "outputId": "b9e85b22-89ea-4689-e6f9-d2531ade6c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "#changing the working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAg6sEoe06IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy API and download dataset into your drive\n",
        "!kaggle datasets download -d snap/amazon-fine-food-reviews"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qci5ABsb1RZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unzipping the zip files and deleting the zip files\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Tkr7qW2EJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #In order to perform machine learning on text, we need to transform our documents into vector representations\n",
        "#  such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization, \n",
        "#    and is an essential first step toward language-aware analysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgw1r961_Eu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read sql file with pandas \n",
        "import sqlite3                                               # to call query \n",
        "from  sqlalchemy import create_engine                       # to create the connection \n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-TW9wJS9amv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "182752ca-9b15-4ecd-8a8c-fce58bb4a46d"
      },
      "source": [
        "\n",
        "# df = pd.read_csv('Reviews.csv')\n",
        "# print(df.shape)\n",
        "# #df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(568454, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIz5g_k6GDNy",
        "colab_type": "text"
      },
      "source": [
        "#How to load very big data with pandas \n",
        "source: https://www.youtube.com/watch?v=xKMyk4wDHnQ\n",
        "\n",
        "Three  steps:\n",
        "\n",
        "1. create a connector to a database\n",
        "       # csv_databse = create_engine('sqlite://csv_database.db')\n",
        "2. Building the database(load the csv file ) by chunking\n",
        "       # In this case we we already have .sql file.So, this skiping  this step\n",
        "3. construct the Pandas dataframe from database by sql query\n",
        "        pd.read_sql_query(\"select statement\")   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUvzkig5_2V2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc519d60-6ef5-4125-b835-0cb80b667c6b"
      },
      "source": [
        "# Read sqlite query results into a pandas DataFrame\n",
        "con = sqlite3.connect(\"database.sqlite\")\n",
        "sql_df = pd.read_sql_query(\"SELECT * from Reviews\", con)\n",
        "\n",
        "# Verify that result of SQL query is stored in the dataframe\n",
        "#print(df.head())\n",
        "print(sql_df.shape)\n",
        "con.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(568454, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1pn0i-7AyHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}