{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.kaggle.com/shashanksai/text-preprocessing-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting re\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 353, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/wheel.py\", line 749, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/usr/lib/python3/dist-packages/pip/req/req_set.py\", line 554, in _prepare_file\n",
      "    require_hashes\n",
      "  File \"/usr/lib/python3/dist-packages/pip/req/req_install.py\", line 278, in populate_link\n",
      "    self.link = finder.find_requirement(self, upgrade)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 465, in find_requirement\n",
      "    all_candidates = self.find_all_candidates(req.name)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 423, in find_all_candidates\n",
      "    for page in self._get_pages(url_locations, project_name):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 568, in _get_pages\n",
      "    page = self._get_page(location)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 683, in _get_page\n",
      "    return HTMLPage.get_page(link, session=self.session)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 795, in get_page\n",
      "    resp.raise_for_status()\n",
      "  File \"/usr/share/python-wheels/requests-2.18.4-py2.py3-none-any.whl/requests/models.py\", line 935, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://pypi.org/simple/re/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/8e/86/c14387d6813ebadb7bf61b9ad270ffff111c8b587e4d266e07de774e385e/pandas-1.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pytz>=2017.2 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.6.1 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.13.3 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting six>=1.5 (from python-dateutil>=2.6.1->pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: pytz, six, python-dateutil, numpy, pandas\n",
      "Successfully installed numpy-1.18.4 pandas-1.0.4 python-dateutil-2.8.1 pytz-2020.1 six-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/e6/54aaaafd0b87f51dfba92ba73da94151aa3bc179e5fe88fc5dfb3038e860/seaborn-0.10.1-py3-none-any.whl (215kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 832kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.0.1 (from seaborn)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting matplotlib>=2.1.2 (from seaborn)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.4MB 90kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas>=0.22.0 (from seaborn)\n",
      "  Using cached https://files.pythonhosted.org/packages/8e/86/c14387d6813ebadb7bf61b9ad270ffff111c8b587e4d266e07de774e385e/pandas-1.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.13.3 (from seaborn)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cycler>=0.10 (from matplotlib>=2.1.2->seaborn)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib>=2.1.2->seaborn)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 3.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib>=2.1.2->seaborn)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting python-dateutil>=2.1 (from matplotlib>=2.1.2->seaborn)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting pytz>=2017.2 (from pandas>=0.22.0->seaborn)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Collecting six (from cycler>=0.10->matplotlib>=2.1.2->seaborn)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, scipy, six, cycler, pyparsing, kiwisolver, python-dateutil, matplotlib, pytz, pandas, seaborn\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 numpy-1.18.4 pandas-1.0.4 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 scipy-1.4.1 seaborn-0.10.1 six-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrent types of   stemmers :\n",
    "\n",
    "https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "Stemming and lemmatization: \n",
    "https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                                                         #Stopwords corpus\n",
    "from nltk.stem import PorterStemmer                                               # Porter Stemmer\n",
    "from nltk.stem import SnowballStemmer                                              # Portor 2.0 \n",
    "from sklearn.feature_extraction.text import CountVectorizer                       #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer                       #For TF-IDF\n",
    "from gensim.models import Word2Vec                                               #For Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/shiv/Dataset/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f50a192e0b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data = df, y = df.Score.value_counts(), x = df.Score.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_removed = df[df['Score']!=3]       #Neutral reviews removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'positive'\n",
    "    return 'negative'\n",
    "\n",
    "score_upd = df_score_removed['Score']\n",
    "t = score_upd.map(partition)\n",
    "df_score_removed['Score']=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = df_score_removed.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final_data[final_data['HelpfulnessNumerator'] <= final_data['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X = final['Text']  \n",
    "final_y = final['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming- Converting the words into their base word or stem word ( Ex - tastefully, tasty, these words are converted to stem word called 'tasti'). This reduces the vector dimension because we dont consider all similar words\n",
    "\n",
    "Stopwords - Stopwords are the unnecessary words that even if they are removed the sentiment of the sentence dosent change.\n",
    "\n",
    "Ex - This pasta is so tasty ==> pasta tasty ( This , is, so are stopwords so they are removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about', \"you'd\", 'other', 'each', 'i', 'these', 'because', 'same', 'so', 'isn', 'further', 'didn', 'ain', \"mightn't\", 'such', 'has', 'you', 'there', 'with', 'out', 've', \"needn't\", 'where', 'any', 'your', 'up', 'that', 'our', 'its', \"doesn't\", 'over', 'here', 'during', 'which', 'all', 't', 'down', 'theirs', 'more', 'this', 'into', 'yourself', \"you'll\", 'from', 'both', 'was', 'wouldn', \"aren't\", \"should've\", 'ours', 'having', 'under', 'after', 'while', 'to', 'should', 'is', 'hasn', 'at', 'or', 'mightn', 'against', 'the', 'me', 'shouldn', 'what', 'had', 'as', 'doesn', \"that'll\", \"hadn't\", 'in', 'when', 'he', 'hers', 'an', 'will', 'who', 'aren', 'don', 'until', 'hadn', 'than', 'shan', 'her', 'of', 'now', 'do', 'does', 'herself', 'before', 'some', 'between', \"mustn't\", 'no', \"weren't\", 'own', 'my', \"shouldn't\", 'and', 'mustn', 'again', 'been', \"she's\", 'too', 'not', 'themselves', 'how', 'on', 'few', 'being', 'm', 're', 's', 'very', 'll', 'am', \"didn't\", 'were', 'him', \"don't\", \"wouldn't\", 'below', 'she', 'wasn', 'won', 'be', 'most', 'couldn', 'haven', 'whom', 'did', 'd', 'ourselves', 'needn', 'them', 'their', 'itself', \"won't\", 'but', 'ma', \"you've\", \"shan't\", \"it's\", 'only', 'myself', 'are', 'his', 'then', \"isn't\", 'yourselves', 'himself', 'can', \"haven't\", 'above', 'just', 'once', \"you're\", 'they', 'a', 'yours', 'by', 'for', 'why', 'y', \"wasn't\", 'those', 'it', 'off', 'o', 'weren', 'doing', 'have', 'we', 'nor', \"couldn't\", \"hasn't\", 'through', 'if'}\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english')) \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-766d892debf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_X\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# Converting to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcleanr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<.*?>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#Removing HTML tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[?|!|\\'|\"|#]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for sentence in final_X:\n",
    "    sentence = sentence.lower()                 # Converting to lowercase\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #Removing Punctuations\n",
    "    \n",
    "    words = [snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "    temp.append(words)\n",
    "    \n",
    "final_X = temp    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Snowball stemmer (Porter 2.0)\n",
    "#https://www.nltk.org/_modules/nltk/stem/snowball.html\n",
    "\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "print(\" \".join(SnowballStemmer.languages)) # See which languages are supported\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemmer = SnowballStemmer(\"german\") # Choose a language\n",
    "stemmer.stem(\"Autobahnen\") # Stem a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P r o d u c t   a r r i v e d   l a b e l e d   a s   J u m b o   S a l t e d   P e a n u t s . . . t h e   p e a n u t s   w e r e   a c t u a l l y   s m a l l   s i z e d   u n s a l t e d .   N o t   s u r e   i f   t h i s   w a s   a n   e r r o r   o r   i f   t h e   v e n d o r   i n t e n d e d   t o   r e p r e s e n t   t h e   p r o d u c t   a s   \" J u m b o \" .\n"
     ]
    }
   ],
   "source": [
    "#replacing commas with whitespace\n",
    "sent = []\n",
    "for row in final_X:\n",
    "    sequ = ''\n",
    "    for word in row:\n",
    "        sequ = sequ + ' ' + word\n",
    "    sent.append(sequ)\n",
    "\n",
    "final_X = sent\n",
    "print(final_X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks of BoW/ Binary BoW\n",
    "\n",
    "Our main objective in doing these text to vector encodings is that similar meaning text vectors should be close to each other, but in some cases this may not possible for Bow\n",
    "\n",
    "For example, if we consider two reviews This pasta is very tasty and This pasta is not tasty after stopwords removal both sentences will be converted to pasta tasty so both giving exact same meaning.\n",
    "\n",
    "The main problem is here we are not considering the front and back words related to every word, here comes Bigram and Ngram techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BI-GRAM BOW\n",
    "\n",
    "Considering pair of words for creating dictionary is Bi-Gram , Tri-Gram means three consecutive words so as NGram.\n",
    "\n",
    "CountVectorizer has a parameter ngram_range if assigned to (1,2) it considers Bi-Gram BoW\n",
    "\n",
    "But this massively increases our dictionary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_B_X = final_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#how CounVectorizer works \n",
    "#Source: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.', \n",
    "     'Is this the first document?',\n",
    "         ]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "#output \n",
    "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "print(X.toarray())\n",
    "'''\n",
    "#output \n",
    "\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    "'''\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "#output  \n",
    "'''\n",
    "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n",
    "'''\n",
    "print(X2.toarray())\n",
    "\n",
    "#output \n",
    "'''\n",
    "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4e4f40930999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mBigram_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_B_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBigram_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBigram_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2512439\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shiv/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1199\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shiv/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "Bigram_data = count_vect.fit_transform(final_B_X)\n",
    "print(Bigram_data[1])\n",
    "print(Bigram_data[0,2512439])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Bigram_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-86052fcc1824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBigram_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2512439\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Bigram_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(Bigram_data[0,2512439])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency it makes sure that less importance is given to most frequent words and also considers less frequent words.\n",
    "\n",
    "Term Frequency is number of times a particular word(W) occurs in a review divided by totall number of words (Wr) in review. The term frequency value ranges from 0 to 1.\n",
    "\n",
    "Inverse Document Frequency is calculated as log(Total Number of Docs(N) / Number of Docs which contains particular word(n)). Here Docs referred as Reviews.\n",
    "\n",
    "TF-IDF is TF * IDF that is (W/Wr)*LOG(N/n)\n",
    "\n",
    "Using scikit-learn's tfidfVectorizer we can get the TF-IDF.\n",
    "\n",
    "So even here we get a TF-IDF value for every word and in some cases it may consider different meaning reviews as similar after stopwords removal. so to over come we can use BI-Gram or NGram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ead11b840783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shiv/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \"\"\"\n\u001b[1;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shiv/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1199\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shiv/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "final_tf = final_X\n",
    "tf_idf = TfidfVectorizer(max_features=5000)\n",
    "tf_data = tf_idf.fit_transform(final_tf)\n",
    "print(tf_data[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
